from langgraph.graph import StateGraph, END, START
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from typing import TypedDict, List
import json
import httpx
import os
from dotenv import load_dotenv
import asyncio

# Load environment variables from .env
load_dotenv()

# Initialize API keys from .env
openai_api_key = os.getenv("OPENAI_API_KEY")
serper_api_key = os.getenv("SERPER_API_KEY")

# Validate API keys
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY not found in .env")
if not serper_api_key:
    raise ValueError("SERPER_API_KEY not found in .env")

# Initialize LLM with API key
llm = ChatOpenAI(model="gpt-4", temperature=0.0, api_key=openai_api_key)


# State definition
class ResearchState(TypedDict):
    goal: str
    queries: List[str]
    results: List[dict]


# Serper API helper
async def serper_search(query: str, api_key: str) -> dict:
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "https://google.serper.dev/search",
            headers={"X-API-KEY": api_key},
            json={"q": query},
            timeout=10
        )
        return response.json()


# Nodes
def planner(state: ResearchState) -> dict:
    goal = state.get("goal", "")
    if not goal:
        return {"queries": []}
    prompt = PromptTemplate.from_template("""
    Create 3 Google search queries to research: {goal}
    Return ONLY a JSON array like ["query1", "query2", "query3"]
    """)
    response = llm.invoke(prompt.format(goal=goal))
    return {"queries": json.loads(response.content)}


async def searcher(state: ResearchState) -> dict:
    if not state.get("queries"):
        return {"results": []}
    formatted_results = []
    for query in state["queries"]:
        try:
            data = await serper_search(query, serper_api_key)
            formatted_results.append({
                "query": query,
                "top_result": data["organic"][0] if data.get("organic") else None,
                "knowledge_graph": data.get("knowledgeGraph")
            })
        except Exception as e:
            formatted_results.append({"query": query, "error": str(e)})
    return {"results": formatted_results}


# Graph construction
workflow = StateGraph(ResearchState)
workflow.add_node("plan", planner)
workflow.add_node("search", searcher)
workflow.set_entry_point("plan")
workflow.add_edge("plan", "search")
workflow.add_edge("search", END)
graph = workflow.compile()


# Test function (async to support searcher)
async def run_agent(goal: str):
    result = await graph.ainvoke({"goal": goal, "queries": [], "results": []})
    return result


# Run the test
if __name__ == "__main__":
    # Test with a sample goal
    goal = "Techniques for compressing LLMs under 1B parameters"
    print(f"Testing with goal: {goal}")
    result = asyncio.run(run_agent(goal))

    # Print the results
    print("Queries generated by planner:", result["queries"])
    print("Search results:")
    for res in result["results"]:
        print(res)